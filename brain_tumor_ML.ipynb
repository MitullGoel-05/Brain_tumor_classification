{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37ba43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MLProject(Guided): Brain Tumor Cancer Prediction\n",
    "#-----------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_column', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Exploratory data analysis(EDA)\n",
    "#Load the dataset and do some quick exploratory data analysis.\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\dell\\\\OneDrive\\\\Desktop\\\\EXTRAS\\\\IIT KANPUR MACHINE LEARNING(STP) SUMMER 2025\\\\BrainTumorData.csv\", index_col=False)\n",
    "print(\"\\n\\n\\nSample BrainTumor dataset head(5) :- \\n\\n\", data.head(5) )\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nShape of the BrainTumor dataset data.shape = \", end=\"\")\n",
    "print( data.shape)\n",
    "#(569, 33)\n",
    "print(\"\\n\\n\\nBrainTumor data decription : \\n\")\n",
    "print( data.describe() )\n",
    "\n",
    "\n",
    "\n",
    "#Data visualisation and pre-processing\n",
    "#First thing to do is to enumerate the diagnosis column such that M = 1, B = 0.\n",
    "# Then, I set the ID column to be the row-index(=row headings) of the dataframe.\n",
    "# After all, the ID column will not be used for machine learning.\n",
    "\n",
    "\n",
    "print( \"\\n\\n\\ndata.diagnosis.unique() : \" , data.diagnosis.unique() )\n",
    "\n",
    "\n",
    "#Replace M = 1 and B = 0\n",
    "\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "\n",
    "# data['diagnosis'] = data['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "print(\"\\n\\n\\nAfter updation of diagnosis feature: \\n\", data.head() )\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(data['diagnosis'])\n",
    "plt.title('Diagnosis (M=1 , B=0)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = data.set_index('id')\n",
    "print(\"\\n\\n\\nAfter id feature is set as row index: \\n\", data)\n",
    "\n",
    "\n",
    "del data['Unnamed: 32']\n",
    "print(\"\\n\\nAfter Deletion of 'Unnamed: 32' column\\n\", data)\n",
    "\n",
    "\n",
    "\n",
    "#Let's take a look at the number of Benign and Malignant cases from the dataset.\n",
    "# From the output shown below, majority of the cases are benign (0).\n",
    "\n",
    "print(\"\\n\\n\\ndata.groupby('diagnosis').size()\\n\")\n",
    "print(data.groupby('diagnosis').size())\n",
    "\n",
    "#diagnosis\n",
    "#0 357\n",
    "#1 212\n",
    "#dtype: int64\n",
    "\n",
    "#Next, we visualise the data using density plots to get a sense of the data distribution.\n",
    "# From the outputs below, you can see the data shows a general gaussian distribution.\n",
    "\n",
    "data.plot(kind='density', subplots=True, layout=(5,7), sharex=False, legend=False, fontsize=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import cm as cm\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "#cmap = cm.get_cmap('jet', 30)\n",
    "#cax = ax1.imshow(data.corr(), interpolation=\"none\", cmap=cmap)\n",
    "cax = ax1.imshow(data.corr(), interpolation=\"none\")\n",
    "ax1.grid(True)\n",
    "plt.title('Cancer Attributes Correlation')\n",
    "\n",
    "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "\n",
    "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Finally, we'll split the data into predictor variables and target variable,\n",
    "# following by breaking them into train and test sets. We will use 33% of the data as test set.\n",
    "\n",
    "Y = data['diagnosis'].values\n",
    "X = data.drop('diagnosis', axis=1).values\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.33, random_state=21)\n",
    "# 1 2 3 4\n",
    "#Baseline algorithm checking\n",
    "#From the dataset, we will analysis and build a model to predict if a given set of\n",
    "# symptoms lead to a cancerous BrainTumor.\n",
    "# This is a binary classification problem, and a few algorithms are appropriate for use.\n",
    "# Since we do not know which one will perform the best at the point,\n",
    "# we will do a quick test on the few algorithms to get an early indication of how each of them perform.\n",
    "# We will use K-Fold cross validation for each testing.\n",
    "#The following algorithms will be used,\n",
    "#1) Classification and Regression Trees (CART),\n",
    "#2) Support Vector Machines (SVM),\n",
    "#3) # Gaussian Naive Bayes (NB)\n",
    "#4) k-Nearest Neighbors (KNN).\n",
    "\n",
    "models_list = []\n",
    "models_list.append(('CART', DecisionTreeClassifier()))\n",
    "models_list.append(('SVM', SVC()))\n",
    "models_list.append(('NB', GaussianNB()))\n",
    "models_list.append(('KNN', KNeighborsClassifier()))\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "for name, model in models_list:\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    start_Time = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    end_Time = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "print( \"%-10s: %10f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end_Time-start_Time))\n",
    "\n",
    "\n",
    "\n",
    "#CART: 0.912029 (0.039630) (run time: 0.138211)\n",
    "#SVM: 0.619614 (0.082882) (run time: 0.164310)\n",
    "#NB: 0.940773 (0.033921) (run time: 0.019228)\n",
    "#KNN: 0.927729 (0.055250) (run time: 0.027202)\n",
    "\n",
    "#Performance Comparision\n",
    "#------------------------------\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Performance Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#From the initial run, it looks like GaussianNB, KNN and CART performed the best\n",
    "# given the dataset (all above 92% mean accuracy).\n",
    "# Support Vector Machine has a surprisingly bad performance here.\n",
    "# However, if we standardise the input dataset, it's performance should improve.\n",
    "#Evaluation of algorithm on Standardised Data\n",
    "#The performance of the machine learning algorithm could be improved if a\n",
    "# standardised dataset is being used.\n",
    "# The improvement is likely for all the models.\n",
    "# I will use pipelines that standardize the data and build the model for each\n",
    "# fold in the cross-validation test harness.\n",
    "# That way we can get a fair estimation of how each model with standardized data might perform on unseen data.\n",
    "# Standardize the dataset\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledCART', Pipeline([ ('Scaler', StandardScaler()),('CART', DecisionTreeClassifier()) ] )))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC( ))])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "print(\"\\n\\n\\nAccuracies of algorithm after scaled dataset\\n\")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))\n",
    "\n",
    "\n",
    "#ScaledCART: 0.920966 (0.038259) (run time: 0.098808)\n",
    "#ScaledSVM: 0.964879 (0.038621) (run time: 0.073377)\n",
    "#ScaledNB: 0.931932 (0.038625) (run time: 0.027154)\n",
    "#ScaledKNN: 0.958357 (0.038595) (run time: 0.040088)\n",
    "#Notice the drastic improvement of SVM after using scaled data.\n",
    "#Performance Comparison after Scaled Data\n",
    "#----------------------------------------\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Performance Comparison after Scaled Data')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Application of SVC on dataset\n",
    "#Let's fit the SVM to the dataset and see how it performs given the test data.\n",
    "# prepare the model\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "model = SVC()\n",
    "start = time.time()\n",
    "model.fit(X_train_scaled, Y_train) #Training of algorithm using 67% of data\n",
    "end = time.time()\n",
    "print( \"\\n\\nSVM Training Completed. It's Run Time: %f\" % (end-start))\n",
    "\n",
    "#Run Time: 0.004889# estimate accuracy on test dataset\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(\"All predictions done successfully by SVM Machine Learning Algorithms\")\n",
    "print(\"\\n\\nAccuracy score %f\" % accuracy_score(Y_test, predictions))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"confusion_matrix = \\n\")\n",
    "print( confusion_matrix(Y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
